{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리 (Natural Language Processing) 의 소개\n",
    "시계열 데이터의 핵심, 바로 **자연어 처리**에 대해서 알아보도록 하겠습니다. 지금까지는 **시계열 데이터**에 대해서 알아보았고 해당 데이터의 수치화 방법에 대해서 알아보았습니다. 자연어의 경우도 똑같이 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: 자연어 처리란?\n",
    "A: 인간의 언어 현상을 컴퓨터와 같은 기계를 이용해서 모사 할수 있도록 연구하는 분야 (위키피디아)\n",
    "\n",
    "Simply.. 인간이 구사하는 언어를 컴퓨터가 이해하고 구사하게 하는 프로그램 입니다.\n",
    "\n",
    "### Q: 언어는 어떻게 수치화 하나요?\n",
    "여러가지 방법이 있습니다.\n",
    "\n",
    "- One-hot Vector\n",
    "- Word Embedding\n",
    "\n",
    "이 대표적이라고 보면 됩니다. 하나하나 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Vector\n",
    "현재 인간이 구사할 수 있는 단어가 다음 뿐이라고 가정해봅시다.\n",
    "\n",
    "1. 안녕\n",
    "2. 나는\n",
    "3. HAI\n",
    "4. 를\n",
    "5. 사랑해\n",
    "\n",
    "위와 같은 단어의 모임을 Vocabulary 라고 합니다. 흠... 그럼 다음과 같은 5개의 단어를 어떻게 하면 컴퓨터가 수학적으로 알아듣게 할까요? 이런방법이 있겠지요?\n",
    "\n",
    "**안녕** 이라는 단어를 [1, 0, 0, 0, 0] <br>\n",
    "**나는** 이라는 단어를 [0, 1, 0, 0, 0] <br>\n",
    "... <br>\n",
    "**사랑해** 이라는 단어를 [0, 0, 0, 0, 1] <br>\n",
    "라고 표현을 해봅시다.\n",
    "\n",
    "위와 같이 표현하면 모든 단어가 각각이 다른 행렬 (같은 말로는 벡터) 를 가지게 되겠죠? 즉, 총 알고있는 단어의 갯수가 N개 일때, (1 by N) 벡터로 단어를 표현할 수 있는 겁니다.<br>\n",
    "여기서, 한 column 만 1 이고 나머지 column은 0 이라는 특징을 따 \"One-hot Vector\" 라고 부릅니다.\n",
    "\n",
    "한번 One-hot Vector 를 만들어보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수업자료 깃허브 Repository를 다운받아준다\n",
    "!rm -rf ./NLP_2020\n",
    "!git clone https://github.com/HanyangTechAI/NLP_2020.git\n",
    "!rm -rf ./NLP_2020/.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 (인생을 편하게 해주는 미리 만들어놓은 프로그램이라고 보면 된다) 를 설치시킨다\n",
    "!cat ./NLP_2020/lab01_all_about_sequence/requirements.txt\n",
    "!echo '----------------------------------'\n",
    "!pip install -r ./NLP_2020/lab01_all_about_sequence/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Vector 를 만들어주는 함수를 구현해봅시다\n",
    "# Input 으로는 단어가 있는 List 를 받고\n",
    "# Output 으로는 단어와 One-hot-Vector가 Mapping 된 Dictionary 를 반환합니다.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gen_one_hot(word_list):\n",
    "    length = len(word_list)\n",
    "    result_dict = {}\n",
    "    \n",
    "    for idx, word in enumerate(word_list):\n",
    "        one_hot_vec = np.zeros(length)  # shape은 (length, ) 이고 값은 전부 0인 행렬(벡터)를 만듭니다\n",
    "        one_hot_vec[idx] = 1  # 해당 word에 해당하는 one_hot vector를 만들었습니다.\n",
    "        \n",
    "        result_dict[word] = one_hot_vec\n",
    "    \n",
    "    return result_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 만든 단어의 모임을 List 형으로 구성해봅시다\n",
    "cluster_of_words = ['안녕', '나는', 'HAI', '를', '사랑해']\n",
    "print(cluster_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 cluster_of_words의 one-hot vector를 만듭시다\n",
    "dictionary = gen_one_hot(cluster_of_words)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 Dictionary를 어떻게 이요할수 있을까요? Toy 코드를 한번 짜보죠.\n",
    "\n",
    "## 사용법 ##\n",
    "# 1. Cluster of words 에 있는 단어를 입력하면 One-hot Vector가 출력된다\n",
    "# 2. 이외의 단어를 입력하면 프로그램이 종료된다\n",
    "\n",
    "while True:\n",
    "    input_word = input(\"단어를 입력하시오: \")\n",
    "    if input_word in dictionary:\n",
    "        print(dictionary[input_word])\n",
    "    else:\n",
    "        print(\"Ending Loop\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "One-hot Vector로 모든 단어를 표현할 수 있는데 왜 Word Embedding 이 필요한거죠? One-hot Vector의 downside 에 대해서 알아보겠습니다.\n",
    "\n",
    "<b>Downside of One-hot Vector</b>\n",
    "1. 단어의 수가 무수히 많아지면 Vector의 Shape 또한 무수히 커진다\n",
    "2. 단어의 수가 변하면 Shape 또한 변한다\n",
    "3. Boolean 값으로 나머지 값들은 낭비된다. (매우 Sparse한 vector)\n",
    "\n",
    "**어떻게 해결할 수 있을까?** <br>\n",
    "단어들을 One-hot Vector 가 아닌 실수로 이루어져있는 Vector로 표현한다. 여러가지 Semi-Supervised 학습 알고리즘으로, 각 단어를 가장 잘 대표하는 실수형 벡터를 뽑아 사용한다.\n",
    "\n",
    "여기까지 읽으면 **절대** 감이 안 잡힙니다. (이해됐으면 천재 인정) <br>\n",
    "**먼저** Word Embedding 을 Pytorch 프레임워크에서 어떻게 사용하는지 알아보도록 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 프레임워크인 torch를 import 해줍니다\n",
    "import torch\n",
    "\n",
    "# Pytorch 프레임워크 내부에 Neural Network 모델을 만들수 있는 torch.nn 을 import 해줍니다\n",
    "import torch.nn as nn  #  as 뒤에 nn 은, torch.nn 대신 nn 이라고 짧게 쓰겠다는 의미!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster of Words 에 뭐가 있는 확인차\n",
    "# 다시 출력해줍니다\n",
    "print(cluster_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '안녕' 은 0\n",
    "# '나는' 은 1\n",
    "#...\n",
    "# 으로 매핑 되는 word_dict (type: Dictionary) 를 만들어줍니다\n",
    "word_dict = {word : index for index, word in enumerate(cluster_of_words)}\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '안녕' , '나는', '사랑해'\n",
    "# 라는 단어의 Sequence 를\n",
    "# 위 word_dict 를 이용해 Index 의 시퀀스로 나타내 봅시다\n",
    "\n",
    "# 1. 먼저 split 메서드를 통해 string 형의 문장을 단어 단위로 쪼개줍니다\n",
    "sentence = \"안녕 나는 사랑해\"\n",
    "word_list = sentence.split(\" \")  # \" \"을 기준으로 split해 list 형으로 반환\n",
    "print('word_list : {}'.format(word_list))\n",
    "\n",
    "# 2. word_list 안의 단어들을 각각의 index 로 mapping 해봅시다\n",
    "index_list = list(map(lambda word: word_dict[word], word_list))\n",
    "print('index_list : {}'.format(index_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금 까지\n",
    "# 문장 -> 단어들의 리스트 -> 각 단어에 해당하는 Index 들의 리스트\n",
    "# 로 데이터를 변환해줬습니다.\n",
    "#####\n",
    "# 마지막으로 Pytorch 가 이해할 수 있도록\n",
    "# torch.LongTensor 형으로 바꿔주면 끝\n",
    "\n",
    "input_tensor = torch.LongTensor(index_list)\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 Embedding 레이어를 만들어 봅시다\n",
    "\n",
    "number_of_class = len(cluster_of_words)  # 총 단어들의 갯수\n",
    "embedding_dimension = 10  # 각 단어당 Tensor 의 크기 -> 선형대수에서는 dimension 으로 표현\n",
    "\n",
    "word_emb = nn.Embedding(num_embeddings=number_of_class,\n",
    "                       embedding_dim=embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 word_emb 을 통해 단어들에 상응하는 벡터를 출력해봅시다\n",
    "\n",
    "# Input 텐서와 임베딩 모델을 GPU 에 올려줍니다 (가속화를 위해)\n",
    "input_tensor = input_tensor.to('cuda')\n",
    "word_emb = word_emb.to('cuda')\n",
    "\n",
    "# Input 텐서를 임베딩 모델에 넣고, Output 텐서를 받아줍니다\n",
    "output_tensor = word_emb(input_tensor)\n",
    "\n",
    "# Output 텐서를 cpu로 내려주고, numpy로 변환해줍니다\n",
    "output_tensor = output_tensor.to('cpu').detach().numpy()\n",
    "\n",
    "# shape 와 값을 출력해줍니다\n",
    "print('shape: {}'.format(output_tensor.shape))\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위의 내용을 정리를 해보겠습니다** <br>\n",
    "\n",
    "1. **'안녕 나는 사랑해'** 라는 문장을 **[0, 1, 4]** 로 나타내줬습니다\n",
    "2. **nn.Embedding** 이라는 단어 임베딩 레이어에 **[0, 1, 4]** 를 입력했습니다\n",
    "3. **nn.Embedding** 내부에서는, [0번, 1번, 4번] 에 해당되는 Tensor를 반환하여, shape은 [시퀀스의 길이 , 텐서의 Dimension] 의 텐서를 반환합니다.\n",
    "\n",
    "### 즉\n",
    "**'안녕 나는 사랑해'** 를\n",
    "\n",
    "안녕 : [a1, a2, a3, ... , an] <br>\n",
    "나는 : [b1, b2, b3, ... , bn] <br>\n",
    "사랑해 : [c1, c2, c3, ... , cn] <br>\n",
    "\n",
    "이런 행렬로 나타냈다고 이해하면 됩니다.\n",
    "\n",
    "### 의문점 !!\n",
    "\n",
    "**Q:** **nn.Embedding** 에서 출력되는 벡터의 값이 정말 그 단어를 잘 대표할수 있다고 말할 수 있나? <br>\n",
    "**A:** **아니요.** 벡터가 해당 단어를 잘 대표하기 위해 **'학습'** 이라는 과정을 거치게 됩니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
