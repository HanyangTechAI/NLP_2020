{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Image Representation\n",
    "Q: Time 도메인의 sound wave 를 Image와 같은 2-D Timestep-Frequency 도메인으로 나타냈을 때 어떤 장점들이 있을까?\n",
    "\n",
    "A: **Vocal(Sound Source)-Isolation** 이라는 Task에 대해 매우 effective하게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocal Isolation 이란?\n",
    "음원이 있을때 Vocal(목소리)를 제외한 나머지 음원만 남기는 Process를 말합니다. 어떻게 목소리를 제거할 수 있을까요?\n",
    "\n",
    "**기존 방법**: 목소리에 해당되는 음역대(Frequency)를 제거한다.\n",
    "\n",
    "**기존 방법의 단점**: 목소리와 같은 배경음의 음역대도 함께 제거된다. 언제까지나 눈대중으로 하는 것이기 때문에 완벽하지 않다.\n",
    "\n",
    "**AI를 만들줄 아는 우리들의 방법**: 목소리에 해당되는 음역대(Frequency)를 AI가 알아서 제거하게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수업자료 깃허브 Repository를 다운받아준다\n",
    "!rm -rf ./NLP_2020\n",
    "!git clone https://github.com/HanyangTechAI/NLP_2020.git\n",
    "!rm -rf ./NLP_2020/.git\n",
    "\n",
    "!rm -rf ./DeepIsolation\n",
    "!git clone https://github.com/litcoderr/DeepIsolation.git\n",
    "!rm -rf ./DeepIsolation/.git\n",
    "\n",
    "# 필요한 패키지 (인생을 편하게 해주는 미리 만들어놓은 프로그램이라고 보면 된다) 를 설치시킨다\n",
    "!cat ./NLP_2020/lab02_sequence_meets_computer_vision/requirements.txt\n",
    "!echo '----------------------------------'\n",
    "!pip install -r ./NLP_2020/lab02_sequence_meets_computer_vision/requirements.txt\n",
    "\n",
    "!pip install youtube_dl\n",
    "!pip install pydub\n",
    "!apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_dl\n",
    "\n",
    "def download(link):\n",
    "  ydl_opts = {\n",
    "      'format': 'bestaudio/best',\n",
    "      'postprocessors': [{\n",
    "          'key': 'FFmpegExtractAudio',\n",
    "          'preferredcodec': 'mp3',\n",
    "          'preferredquality': '192',\n",
    "      }],\n",
    "  }\n",
    "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    result = ydl.extract_info(link,download=False)\n",
    "    if 'entries' in result:\n",
    "      video = result['entries'][0]\n",
    "      return 0\n",
    "    else:\n",
    "      video = result\n",
    "      title = \"{}-{}.mp3\".format(video[\"title\"],video['id'])\n",
    "      ydl.download([link])\n",
    "      return title\n",
    "    \n",
    "link = input(\"유튜브 링크를 입력하시오: \")\n",
    "file_path = download(link)\n",
    "file_path = file_path.replace('/', '_')\n",
    "print('------------------------\\n{} has been download'.format(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "import pydub\n",
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "\n",
    "def read_mp3(file_path, as_float = True):\n",
    "    \"\"\"\n",
    "    Read an MP3 File into numpy data.\n",
    "    :param file_path: String path to a file\n",
    "    :param as_float: Cast data to float and normalize to [-1, 1]\n",
    "    :return: Tuple(rate, data), where\n",
    "        rate is an integer indicating samples/s\n",
    "        data is an ndarray(n_samples, 2)[int16] if as_float = False\n",
    "            otherwise ndarray(n_samples, 2)[float] in range [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    path, ext = os.path.splitext(file_path)\n",
    "    assert ext=='.mp3'\n",
    "    mp3 = pydub.AudioSegment.from_mp3(file_path)\n",
    "    _, path = tempfile.mkstemp()\n",
    "    mp3.export(path, format=\"wav\")\n",
    "    rate, data = scipy.io.wavfile.read(path)\n",
    "    os.remove(path)\n",
    "    if as_float:\n",
    "        data = data/(2**15)\n",
    "    return rate, data\n",
    "\n",
    "def plot(wave, sample_rate, start_sec, end_sec):\n",
    "    start_index = int(float(start_second) * sample_rate)\n",
    "    end_index = int(float(end_second) * sample_rate)\n",
    "    \n",
    "    plt.plot(wave[start_index:end_index])\n",
    "    plt.ylabel('wave')\n",
    "    plt.axis([None,None,-1,1])\n",
    "    plt.show()\n",
    "\n",
    "sample_rate, wave_data = read_mp3(file_path='./{}'.format(file_path))\n",
    "# Mono 오디오 파일로 변환\n",
    "wave_data = wave_data.sum(axis=1) / 2\n",
    "\n",
    "print('wave shape: {}'.format(wave_data.shape))\n",
    "print('sample_rate: {}'.format(sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepIsolation API를 통해 학습된 모델을 Import 받아봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./DeepIsolation\")\n",
    "\n",
    "from DeepIsolation import API\n",
    "from DeepIsolation.modules.util.stft import STFTConverter\n",
    "\n",
    "api = API.Model(n_gpu=1)\n",
    "model = api.getModel()\n",
    "\n",
    "converter  = STFTConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 Spectrogram 전체를 집어넣으면 용량이 너무 크기 때문에\n",
    "# segment로 잘라서 모델에 집어넣는 function 을 만들어준다\n",
    "\n",
    "def isolate(wave, rate):\n",
    "    # 10초 단위로 끊어 model 에 집어넣도록 하자\n",
    "    interval_sec = 5\n",
    "    length_sec = wave.shape[0] // rate  # 전체 길이 in seconds\n",
    "    \n",
    "    result = []\n",
    "    filter_vector = []\n",
    "    for i in tqdm(range(length_sec//interval_sec - 1), desc='Isolating MR'):\n",
    "        start_sec = i * interval_sec  # 시작 초\n",
    "        end_sec = (i+1) * interval_sec  # 끝 초\n",
    "        \n",
    "        start_tick = start_sec * rate  # 시작 tick index\n",
    "        end_tick = end_sec * rate  # 끝 tick index\n",
    "        \n",
    "        # STFT 변환을 start_tick에서 end_tick 까지 해준다\n",
    "        magnitude, phase = converter.get_stft(wave[start_tick:end_tick])\n",
    "        \n",
    "        # magnitude 를 pytorch Tensor로 변환해준다\n",
    "        # shape: [batch_size(1), 1, time, freq]\n",
    "        input_tensor = torch.FloatTensor(magnitude).to('cuda').unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # 모델에 집어넣어준다\n",
    "        # output: [batch_size(1), 1, time, freq]\n",
    "        estimated_magnitude, filter_vec = model(input_tensor)\n",
    "        \n",
    "        result.append(estimated_magnitude.cpu().detach().numpy()[0, 0, :, :])\n",
    "        filter_vector.append(filter_vec.cpu().detach().numpy()[0, 0, :, :])\n",
    "    \n",
    "    result = np.concatenate(result)\n",
    "    filter_vector = np.concatenate(filter_vector)\n",
    "    return result, filter_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_magnitude, filter_vec = isolate(wave_data, sample_rate)\n",
    "print('estimated_magnitude shape: {}'.format(estimated_magnitude.shape))\n",
    "print('filter_vector shape: {}'.format(filter_vec.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_magnitude, original_phase = converter.get_stft(wave_data)\n",
    "# estimated magnitude 가 clipping 됨을 가만하여 길이를 estimated 와 맞춰주자\n",
    "original_magnitude = original_magnitude[:estimated_magnitude.shape[0], :]\n",
    "original_phase = original_phase[:estimated_magnitude.shape[0], :]\n",
    "\n",
    "print('original_magnitude_shape: {}'.format(original_magnitude.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram이 어떻게 변화 했는지 알아봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.xlabel('original_magnitude')\n",
    "plt.imshow(original_magnitude[2000:4000, :])\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.xlabel('estimated_magnitude')\n",
    "plt.imshow(estimated_magnitude[2000:4000, :])\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.xlabel('filter')\n",
    "plt.imshow(filter_vec[2000:4000, :])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 Sound Wave 는 어떻게 변화하는지 알아봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_isolated_spectrogram = np.stack((estimated_magnitude, original_phase))\n",
    "instrument_isolated_wave = converter.stft2wav(instrument_isolated_spectrogram)\n",
    "\n",
    "print('instrument_isolated_wave shape: {}'.format(instrument_isolated_wave.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파형은 근본적으로 덧셈과 뺄셈이 가능하다.\n",
    "# vocal을 ioslation 하고 싶으면 어떻게 하면 될까?\n",
    "\n",
    "vocal_isolated_wave = wave_data[:instrument_isolated_wave.shape[0]] - instrument_isolated_wave\n",
    "# 모든 source가 포함된 wave_dat에서 instrument만 있는 파형을 빼면 된다\n",
    "print('vocal_isolated_wave shape: {}'.format(vocal_isolated_wave.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tick = 300000\n",
    "end_tick = 500000\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(30)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.plot(wave_data[start_tick:end_tick])\n",
    "plt.xlabel('original')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.ylim(-1, 1)\n",
    "plt.plot(instrument_isolated_wave[start_tick:end_tick])\n",
    "plt.xlabel('instrument isolated')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.ylim(-1, 1)\n",
    "plt.plot(vocal_isolated_wave[start_tick:end_tick])\n",
    "plt.xlabel('vocal isolated')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 음악을 들어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "def clip(wav_type, start_sec, end_sec): # enter(type of wave(\"o\"|\"i\"|\"v\"), start second, end second)\n",
    "    if wav_type == \"original\": # original\n",
    "        temp = wave_data\n",
    "    elif wav_type == \"instrument\": # inst\n",
    "        temp = instrument_isolated_wave\n",
    "    elif wav_type == \"vocal\": # vocal\n",
    "        temp = vocal_isolated_wave\n",
    "        \n",
    "    temp = temp[int(start_sec*sample_rate): int(end_sec*sample_rate)]\n",
    "    \n",
    "    return temp\n",
    "\n",
    "while True:\n",
    "    wav_type = input(\"듣고 싶은 wave type을 입력하시오. (original, instrument, vocal): \")\n",
    "    if wav_type == \"\":\n",
    "        break\n",
    "    \n",
    "    start_sec = float(input(\"시작 초: \"))\n",
    "    end_sec = float(input(\"끝 초: \"))\n",
    "    \n",
    "    clipped_wav = clip(wav_type, start_sec, end_sec)\n",
    "    \n",
    "    print('\\n[{file_path}-{wav_type}] {start_sec}sec - {end_sec}sec\\n'.format(\n",
    "        file_path = file_path,\n",
    "        wav_type = wav_type,\n",
    "        start_sec = start_sec,\n",
    "        end_sec = end_sec\n",
    "    ))\n",
    "    \n",
    "    ipd.display(ipd.Audio(clipped_wav, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
